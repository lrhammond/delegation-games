{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nashpy as nash\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prisoner's Dilemma\n",
    "pd = nash.Game(np.array([[3, 0], [5, 1]]), np.array([[3, 5], [0, 1]]))\n",
    "\n",
    "# Negative Prisoner's Dilemma\n",
    "pd_negative = nash.Game(np.array([[-3, -10], [0, -5]]), np.array([[-3, 0], [-10, -5]]))\n",
    "\n",
    "# Game of Chicken\n",
    "chicken = nash.Game(np.array([[0, -1], [1, -10]]), np.array([[0, 1], [-1, -10]]))\n",
    "\n",
    "# Stug Hunt\n",
    "hunt = nash.Game(np.array([[5, 0], [2, 1]]), np.array([[5, 2], [0, 1]]))\n",
    "\n",
    "# Stug Hunt\n",
    "coop_hunt = nash.Game(np.array([[5, 1], [1, 1]]), np.array([[5, 1], [1, 1]]))\n",
    "\n",
    "# Matching Pennies\n",
    "mp = nash.Game(np.array([[1, -1], [-1, 1]]), np.array([[-1, 1], [1, -1]]))\n",
    "\n",
    "# Battle of the Sexes\n",
    "bos = nash.Game(np.array([[1, 0], [0, 2]]), np.array([[2, 0], [0, 1]]))\n",
    "\n",
    "# Test Game\n",
    "test = nash.Game(np.array([[1, 0.9999], [1, 1.00001]]), np.array([[1, 1.00001], [1, 1]]))\n",
    "\n",
    "games = {\n",
    "    \"Prisoner's dilemma\": pd,\n",
    "    \"Prisoner's dilemma (negative pay-offs)\": pd_negative,\n",
    "    \"Game of Chicken\": chicken,\n",
    "    \"Stug Hut\": hunt,\n",
    "    \"Matching Pennies\": mp,\n",
    "    \"Battle of the Sexes\": bos,\n",
    "    \"Test game\": test}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure-functions definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMBER OF STRATEGIES AND GAME SIZE IS HARDCODED TO FOR NOW (TO 2x2)\n",
    "pure_strategies_2b2 = [[0, 1], [1, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_of_anarchy(game):\n",
    "    # nash equilibria\n",
    "    eqs = list(game.support_enumeration())\n",
    "\n",
    "    # list of expected utilities for both players for pure strategies\n",
    "    pure_outcomes = [game[s_row, s_col]\n",
    "                     for s_row in pure_strategies_2b2 for s_col in pure_strategies_2b2]\n",
    "\n",
    "    # list of expected utilities for both players in nash equilibria\n",
    "    equil_outcomes = [game[s_row, s_col]\n",
    "                      for (s_row, s_col) in eqs]\n",
    "\n",
    "    # we only need to check pure strategies because the maximum is\n",
    "    # necessarily among these\n",
    "    # maximum expected payoff strategy\n",
    "    max_strat = np.argmax(list(map(sum, pure_outcomes)))\n",
    "\n",
    "    # maximum expected payoff nash equil\n",
    "    min_in_equil = np.argmin(list(map(sum, equil_outcomes)))\n",
    "\n",
    "    return sum(pure_outcomes[max_strat]) / sum(equil_outcomes[min_in_equil])\n",
    "\n",
    "\n",
    "def welf_regret(game):\n",
    "    # nash equilibria\n",
    "    eqs = list(game.support_enumeration())\n",
    "\n",
    "    # list of expected utilities for both players for pure strategies\n",
    "    pure_outcomes = [game[s_row, s_col]\n",
    "                     for s_row in pure_strategies_2b2 for s_col in pure_strategies_2b2]\n",
    "\n",
    "    # list of expected utilities for both players in nash equilibria\n",
    "    equil_outcomes = [game[s_row, s_col]\n",
    "                      for (s_row, s_col) in eqs]\n",
    "\n",
    "    # we only need to check pure strategies because the maximum is\n",
    "    # necessarily among these\n",
    "    max_strat = np.argmax(list(map(sum, pure_outcomes)))\n",
    "\n",
    "    min_in_equil = np.argmin(list(map(sum, equil_outcomes)))\n",
    "\n",
    "    return sum(pure_outcomes[max_strat]) - sum(equil_outcomes[min_in_equil])\n",
    "\n",
    "\n",
    "def princ_welf_regret(principal_game, agents_game):\n",
    "    # nash equilibria\n",
    "    eqs = list(agents_game.support_enumeration())\n",
    "\n",
    "    # list of expected utilities for both PRINCIPALS for pure strategies\n",
    "    pure_outcomes = [principal_game[s_row, s_col]\n",
    "                     for s_row in pure_strategies_2b2 for s_col in pure_strategies_2b2]\n",
    "\n",
    "    # list of expected utilities for both PRINCIPALS in nash equilibria of AGENT GAME\n",
    "    equil_outcomes = [principal_game[s_row, s_col]\n",
    "                      for (s_row, s_col) in eqs]\n",
    "\n",
    "    # we only need to check pure strategies because the maximum is\n",
    "    # necessarily among these\n",
    "    max_strat = np.argmax(list(map(sum, pure_outcomes)))\n",
    "\n",
    "    min_in_equil = np.argmin(list(map(sum, equil_outcomes)))\n",
    "\n",
    "    return sum(pure_outcomes[max_strat]) - sum(equil_outcomes[min_in_equil])\n",
    "\n",
    "\n",
    "def epic_horiz(game):\n",
    "    # for measuring horizontal alignment\n",
    "    row_payoffs = game.payoff_matrices[0]\n",
    "    col_payoffs = game.payoff_matrices[1]\n",
    "\n",
    "    # note: uniform distribution assumed\n",
    "    matrix_coeffs = np.corrcoef([row_payoffs.flatten(), col_payoffs.flatten()])\n",
    "    return matrix_coeffs[0][1]\n",
    "\n",
    "\n",
    "def epic_vertic(principal_game, agents_game):\n",
    "    # for measuring vertical alignment\n",
    "\n",
    "    row_payoffs = principal_game.payoff_matrices[0]\n",
    "    col_payoffs = principal_game.payoff_matrices[1]\n",
    "\n",
    "    agent_row_payoffs = agents_game.payoff_matrices[0]\n",
    "    agent_col_payoffs = agents_game.payoff_matrices[1]\n",
    "\n",
    "    # note: uniform distribution assumed\n",
    "    row_matrix_coeffs = np.corrcoef(\n",
    "        [row_payoffs.flatten(), agent_row_payoffs.flatten()])\n",
    "    col_matrix_coeffs = np.corrcoef(\n",
    "        [col_payoffs.flatten(), agent_col_payoffs.flatten()])\n",
    "    return (row_matrix_coeffs[0][1], col_matrix_coeffs[0][1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb(principal_game):\n",
    "    row_payoffs = principal_game.payoff_matrices[0]\n",
    "    col_payoffs = principal_game.payoff_matrices[1]\n",
    "\n",
    "    pert_row_payoffs = np.array([[0, 0], [0, 0]], dtype=float)\n",
    "    pert_col_payoffs = np.array([[0, 0], [0, 0]], dtype=float)\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            payoff = row_payoffs[i][j]\n",
    "            x = random.uniform(-1, 2)\n",
    "            pert_row_payoffs[i][j] = x * payoff\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            payoff = col_payoffs[i][j]\n",
    "            x = random.uniform(-1, 2)\n",
    "            pert_col_payoffs[i][j] = x * payoff\n",
    "\n",
    "    agent_game = nash.Game(pert_row_payoffs, pert_col_payoffs)\n",
    "\n",
    "    return agent_game"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resutls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== PRINCIPALS ======\n",
      "Bi matrix game with payoff matrices:\n",
      "\n",
      "Row player:\n",
      "[[1.      0.9999 ]\n",
      " [1.      1.00001]]\n",
      "\n",
      "Column player:\n",
      "[[1.      1.00001]\n",
      " [1.      1.     ]]\n",
      "\n",
      " ====== AGENTS ======\n",
      "Bi matrix game with payoff matrices:\n",
      "\n",
      "Row player:\n",
      "[[1.      0.9999 ]\n",
      " [1.      1.00001]]\n",
      "\n",
      "Column player:\n",
      "[[1.      1.00001]\n",
      " [1.      1.     ]]\n",
      "\n",
      " ====== MEASURES ======\n",
      "Horizontal Alignment (EPIC): -0.9958634776149621\n",
      "Vertical Alignment (Principal(ROW)-Agent): 1.0\n",
      "Vertical Alignment (Principal(COL)-Agnet): 0.9999999999999998\n",
      "Welfare regret: 1.0000000000065512e-05\n",
      "Cross-game regret: 1.0000000000065512e-05\n",
      "\n",
      " ====== The Grand Equasion ======\n",
      "C = ((VA-R * VA-C) * VC) * (HA * HC)\n",
      "C = ((VA-R[1.0] * VA-C[0.9999999999999999]) * VC[1]) * (HA[0.002068261192518961] * HC[1])\n",
      "\n",
      "C = (VA * VC) * H\n",
      "C = ((VA[0.9999999999999999]) * VC[1]) * H[0.002068261192518961])\n",
      "C = 0.0020682611925189605\n"
     ]
    }
   ],
   "source": [
    "# TODO: add vertical capabilities (epsilon best response)\n",
    "def calculate_measures_and_print(principal_game, agents_game):\n",
    "    print(\"====== PRINCIPALS ======\")\n",
    "    print(principal_game)\n",
    "    print(\"\\n\",\"====== AGENTS ======\")\n",
    "    print(agents_game)\n",
    "\n",
    "    print(\"\\n\",\"====== MEASURES ======\")\n",
    "    print(\"Horizontal Alignment (EPIC): \" + str(epic_horiz(agents_game)))\n",
    "    row_align, col_align = epic_vertic(principal_game, agents_game)\n",
    "    print(\"Vertical Alignment (Principal(ROW)-Agent): \" + str(row_align))\n",
    "    print(\"Vertical Alignment (Principal(COL)-Agnet): \" + str(col_align))\n",
    "    print(\"Welfare regret: \" + str(welf_regret(agents_game)))\n",
    "    print(\"Cross-game regret: \" + str(princ_welf_regret(principal_game, agents_game)))\n",
    "\n",
    "    # The Grand Equasion\n",
    "    # the parentheses are for clarity only, we assume VC to be 1.\n",
    "    # normalze from [-1, 1] to [0, 1] TODO: normalize EPIC\n",
    "    var = (row_align +1) / 2\n",
    "    vac = (col_align +1) / 2\n",
    "    ha = (epic_horiz(agents_game) + 1) /2\n",
    "\n",
    "    print(\"\\n\",\"====== The Grand Equasion ======\")\n",
    "    print(f\"C = ((VA-R * VA-C) * VC) * (HA * HC)\")\n",
    "    print(f\"C = ((VA-R[{var}] * VA-C[{vac}]) * VC[{1}]) * (HA[{ha}] * HC[{1}])\")\n",
    "    print(\"\")\n",
    "    print(f\"C = (VA * VC) * H\")\n",
    "    print(f\"C = ((VA[{var * vac}]) * VC[{1}]) * H[{ha}])\")\n",
    "    print(f\"C = {var * vac * ha}\")\n",
    "\n",
    "\n",
    "calculate_measures_and_print(test, test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
