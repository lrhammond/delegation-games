{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nashpy as nash\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prisoner's Dilemma\n",
    "pd = nash.Game(np.array([[3, 0], [5, 1]]), np.array([[3, 5], [0, 1]]))\n",
    "\n",
    "# Negative Prisoner's Dilemma\n",
    "pd_negative = nash.Game(np.array([[-3, -10], [0, -5]]), np.array([[-3, 0], [-10, -5]]))\n",
    "\n",
    "# Game of Chicken\n",
    "chicken = nash.Game(np.array([[0, -1], [1, -10]]), np.array([[0, 1], [-1, -10]]))\n",
    "\n",
    "# Stug Hunt\n",
    "hunt = nash.Game(np.array([[5, 0], [2, 1]]), np.array([[5, 2], [0, 1]]))\n",
    "\n",
    "# Stug Hunt\n",
    "coop_hunt = nash.Game(np.array([[5, 1], [1, 1]]), np.array([[5, 1], [1, 1]]))\n",
    "\n",
    "# Matching Pennies\n",
    "mp = nash.Game(np.array([[1, -1], [-1, 1]]), np.array([[-1, 1], [1, -1]]))\n",
    "\n",
    "# Battle of the Sexes\n",
    "bos = nash.Game(np.array([[1, 0], [0, 2]]), np.array([[2, 0], [0, 1]]))\n",
    "\n",
    "# Test Game\n",
    "test = nash.Game(np.array([[1, 0.9999], [1, 1.00001]]), np.array([[1, 1.00001], [1, 1]]))\n",
    "\n",
    "games = {\n",
    "    \"Prisoner's dilemma\": pd,\n",
    "    \"Prisoner's dilemma (negative pay-offs)\": pd_negative,\n",
    "    \"Game of Chicken\": chicken,\n",
    "    \"Stug Hut\": hunt,\n",
    "    \"Matching Pennies\": mp,\n",
    "    \"Battle of the Sexes\": bos,\n",
    "    \"Test game\": test}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure-functions definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMBER OF STRATEGIES AND GAME SIZE IS HARDCODED TO FOR NOW (TO 2x2)\n",
    "pure_strategies_2b2 = [[0, 1], [1, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_of_anarchy(game):\n",
    "    # nash equilibria\n",
    "    eqs = list(game.support_enumeration())\n",
    "\n",
    "    # list of expected utilities for both players for pure strategies\n",
    "    pure_outcomes = [game[s_row, s_col]\n",
    "                     for s_row in pure_strategies_2b2 for s_col in pure_strategies_2b2]\n",
    "\n",
    "    # list of expected utilities for both players in nash equilibria\n",
    "    equil_outcomes = [game[s_row, s_col]\n",
    "                      for (s_row, s_col) in eqs]\n",
    "\n",
    "    # we only need to check pure strategies because the maximum is\n",
    "    # necessarily among these\n",
    "    # maximum expected payoff strategy\n",
    "    max_strat = np.argmax(list(map(sum, pure_outcomes)))\n",
    "\n",
    "    # maximum expected payoff nash equil\n",
    "    min_in_equil = np.argmin(list(map(sum, equil_outcomes)))\n",
    "\n",
    "    return sum(pure_outcomes[max_strat]) / sum(equil_outcomes[min_in_equil])\n",
    "\n",
    "\n",
    "def welf_regret(game):\n",
    "    # nash equilibria\n",
    "    eqs = list(game.support_enumeration())\n",
    "\n",
    "    # list of expected utilities for both players for pure strategies\n",
    "    pure_outcomes = [game[s_row, s_col]\n",
    "                     for s_row in pure_strategies_2b2 for s_col in pure_strategies_2b2]\n",
    "\n",
    "    # list of expected utilities for both players in nash equilibria\n",
    "    equil_outcomes = [game[s_row, s_col]\n",
    "                      for (s_row, s_col) in eqs]\n",
    "\n",
    "    # we only need to check pure strategies because the maximum is\n",
    "    # necessarily among these\n",
    "    max_strat = np.argmax(list(map(sum, pure_outcomes)))\n",
    "\n",
    "    min_in_equil = np.argmin(list(map(sum, equil_outcomes)))\n",
    "\n",
    "    return sum(pure_outcomes[max_strat]) - sum(equil_outcomes[min_in_equil])\n",
    "\n",
    "\n",
    "def princ_welf_regret(principal_game, agents_game):\n",
    "    # nash equilibria\n",
    "    eqs = list(agents_game.support_enumeration())\n",
    "\n",
    "    # list of expected utilities for both PRINCIPALS for pure strategies\n",
    "    pure_outcomes = [principal_game[s_row, s_col]\n",
    "                     for s_row in pure_strategies_2b2 for s_col in pure_strategies_2b2]\n",
    "\n",
    "    # list of expected utilities for both PRINCIPALS in nash equilibria of AGENT GAME\n",
    "    equil_outcomes = [principal_game[s_row, s_col]\n",
    "                      for (s_row, s_col) in eqs]\n",
    "\n",
    "    # we only need to check pure strategies because the maximum is\n",
    "    # necessarily among these\n",
    "    max_strat = np.argmax(list(map(sum, pure_outcomes)))\n",
    "\n",
    "    min_in_equil = np.argmin(list(map(sum, equil_outcomes)))\n",
    "\n",
    "    return sum(pure_outcomes[max_strat]) - sum(equil_outcomes[min_in_equil])\n",
    "\n",
    "\n",
    "def epic_horiz(game):\n",
    "    # for measuring horizontal alignment\n",
    "    row_payoffs = game.payoff_matrices[0]\n",
    "    col_payoffs = game.payoff_matrices[1]\n",
    "\n",
    "    # note: uniform distribution assumed\n",
    "    matrix_coeffs = np.corrcoef([row_payoffs.flatten(), col_payoffs.flatten()])\n",
    "    return matrix_coeffs[0][1]\n",
    "\n",
    "\n",
    "def epic_vertic(principal_game, agents_game):\n",
    "    # for measuring vertical alignment\n",
    "\n",
    "    row_payoffs = principal_game.payoff_matrices[0]\n",
    "    col_payoffs = principal_game.payoff_matrices[1]\n",
    "\n",
    "    agent_row_payoffs = agents_game.payoff_matrices[0]\n",
    "    agent_col_payoffs = agents_game.payoff_matrices[1]\n",
    "\n",
    "    # note: uniform distribution assumed\n",
    "    row_matrix_coeffs = np.corrcoef(\n",
    "        [row_payoffs.flatten(), agent_row_payoffs.flatten()])\n",
    "    col_matrix_coeffs = np.corrcoef(\n",
    "        [col_payoffs.flatten(), agent_col_payoffs.flatten()])\n",
    "    return (row_matrix_coeffs[0][1], col_matrix_coeffs[0][1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb(principal_game):\n",
    "    row_payoffs = principal_game.payoff_matrices[0]\n",
    "    col_payoffs = principal_game.payoff_matrices[1]\n",
    "\n",
    "    pert_row_payoffs = np.array([[0, 0], [0, 0]], dtype=float)\n",
    "    pert_col_payoffs = np.array([[0, 0], [0, 0]], dtype=float)\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            payoff = row_payoffs[i][j]\n",
    "            x = random.uniform(-1, 2)\n",
    "            pert_row_payoffs[i][j] = x * payoff\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            payoff = col_payoffs[i][j]\n",
    "            x = random.uniform(-1, 2)\n",
    "            pert_col_payoffs[i][j] = x * payoff\n",
    "\n",
    "    agent_game = nash.Game(pert_row_payoffs, pert_col_payoffs)\n",
    "\n",
    "    return agent_game"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resutls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== PRINCIPALS ======\n",
      "Bi matrix game with payoff matrices:\n",
      "\n",
      "Row player:\n",
      "[[3 0]\n",
      " [5 1]]\n",
      "\n",
      "Column player:\n",
      "[[3 5]\n",
      " [0 1]]\n",
      "\n",
      " ====== AGENTS ======\n",
      "Bi matrix game with payoff matrices:\n",
      "\n",
      "Row player:\n",
      "[[ 5.54372735  0.        ]\n",
      " [ 6.15084049 -0.4437702 ]]\n",
      "\n",
      "Column player:\n",
      "[[ 4.84071445  5.71830372]\n",
      " [-0.         -0.25076428]]\n",
      "\n",
      " ====== MEASURES ======\n",
      "Horizontal Alignment (EPIC): -0.06172994274113411\n",
      "Vertical Alignment (Principal(ROW)-Agent): 0.9243084211394956\n",
      "Vertical Alignment (Principal(COL)-Agnet): 0.9408757046437469\n",
      "Welfare regret: 6.96762380110361\n",
      "Cross-game regret: 2.254129216715841\n",
      "\n",
      " ====== The Grand Equasion ======\n",
      "C = ((VA-R * VA-C) * VC) * (HA * HC)\n",
      "C = ((VA-R[0.9621542105697478] * VA-C[0.9704378523218735]) * VC[1]) * (HA[0.46913502862943296] * HC[1])\n",
      "\n",
      "C = (VA * VC) * H\n",
      "C = ((VA[0.9337108657077537]) * VC[1]) * H[0.46913502862943296])\n",
      "C = 0.4380364737154197\n"
     ]
    }
   ],
   "source": [
    "# TODO: add vertical capabilities (epsilon best response)\n",
    "def calculate_measures_and_print(principal_game, agents_game):\n",
    "    print(\"====== PRINCIPALS ======\")\n",
    "    print(principal_game)\n",
    "    print(\"\\n\",\"====== AGENTS ======\")\n",
    "    print(agents_game)\n",
    "\n",
    "    print(\"\\n\",\"====== MEASURES ======\")\n",
    "    print(\"Horizontal Alignment (EPIC): \" + str(epic_horiz(agents_game)))\n",
    "    row_align, col_align = epic_vertic(principal_game, agents_game)\n",
    "    print(\"Vertical Alignment (Principal(ROW)-Agent): \" + str(row_align))\n",
    "    print(\"Vertical Alignment (Principal(COL)-Agnet): \" + str(col_align))\n",
    "    print(\"Welfare regret: \" + str(welf_regret(agents_game)))\n",
    "    print(\"Cross-game regret: \" + str(princ_welf_regret(principal_game, agents_game)))\n",
    "\n",
    "    # The Grand Equasion\n",
    "    # the parentheses are for clarity only, we assume VC to be 1.\n",
    "    # normalze from [-1, 1] to [0, 1] TODO: normalize EPIC\n",
    "    var = (row_align +1) / 2\n",
    "    vac = (col_align +1) / 2\n",
    "    ha = (epic_horiz(agents_game) + 1) /2\n",
    "\n",
    "    print(\"\\n\",\"====== The Grand Equasion ======\")\n",
    "    print(f\"C = ((VA-R * VA-C) * VC) * (HA * HC)\")\n",
    "    print(f\"C = ((VA-R[{var}] * VA-C[{vac}]) * VC[{1}]) * (HA[{ha}] * HC[{1}])\")\n",
    "    print(\"\")\n",
    "    print(f\"C = (VA * VC) * H\")\n",
    "    print(f\"C = ((VA[{var * vac}]) * VC[{1}]) * H[{ha}])\")\n",
    "    print(f\"C = {var * vac * ha}\")\n",
    "\n",
    "\n",
    "calculate_measures_and_print(pd, perturb(pd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi matrix game with payoff matrices:\n",
      "\n",
      "Row player:\n",
      "[[1 0]\n",
      " [0 2]]\n",
      "\n",
      "Column player:\n",
      "[[2 0]\n",
      " [0 1]]\n",
      "\n",
      "----------vertex_enumeration()---------\n",
      "(array([0., 1.]), array([-2.22044605e-16,  1.00000000e+00]))\n",
      "(array([0.33333333, 0.66666667]), array([0.66666667, 0.33333333]))\n",
      "(array([ 1.00000000e+00, -2.22044605e-16]), array([1., 0.]))\n",
      "\n",
      "----------support_enumeration()---------\n",
      "(array([1., 0.]), array([1., 0.]))\n",
      "(array([0., 1.]), array([0., 1.]))\n",
      "(array([0.33333333, 0.66666667]), array([0.66666667, 0.33333333]))\n",
      "\n",
      "----------lemke_howson_enumeration()---------\n",
      "(array([1., 0.]), array([1., 0.]))\n",
      "(array([0., 1.]), array([0., 1.]))\n",
      "(array([1., 0.]), array([1., 0.]))\n",
      "(array([0., 1.]), array([0., 1.]))\n",
      "\n",
      "----------find_the_equilibrium()---------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.33333333, 0.66666667]), array([0.66666667, 0.33333333]))"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# debugging equilibrium equilibrium search using nashpy methodes\n",
    "game = bos\n",
    "print(game)\n",
    "print(\"\")\n",
    "\n",
    "print(\"----------vertex_enumeration()---------\")\n",
    "eqs = game.vertex_enumeration()\n",
    "for eq in eqs:\n",
    "    print(eq)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"----------support_enumeration()---------\")\n",
    "ggs = game.support_enumeration()\n",
    "for gg in ggs:\n",
    "    print(gg)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"----------lemke_howson_enumeration()---------\")\n",
    "lkms = game.lemke_howson_enumeration()\n",
    "for lkm in lkms:\n",
    "    print(lkm)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# Checking the output of the function\n",
    "print(\"----------find_the_equilibrium()---------\")\n",
    "find_the_equilibrium(game)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
