{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nashpy as nash\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# We assume games are 2x2 for now, hance there are only 2 possible pure strategies.\n",
    "pure_strategies_2b2 = [[0, 1], [1, 0]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prisoner's Dilemma\n",
    "pd = nash.Game(np.array([[3, 0], [5, 1]]), np.array([[3, 5], [0, 1]]))\n",
    "\n",
    "# Negative Prisoner's Dilemma\n",
    "pd_negative = nash.Game(np.array([[-3, -10], [0, -5]]), np.array([[-3, 0], [-10, -5]]))\n",
    "\n",
    "# Game of Chicken\n",
    "chicken = nash.Game(np.array([[0, -1], [1, -10]]), np.array([[0, 1], [-1, -10]]))\n",
    "\n",
    "# Stug Hunt\n",
    "hunt = nash.Game(np.array([[5, 0], [2, 1]]), np.array([[5, 2], [0, 1]]))\n",
    "\n",
    "# Stug Hunt\n",
    "coop_hunt = nash.Game(np.array([[5, 1], [1, 1]]), np.array([[5, 1], [1, 1]]))\n",
    "\n",
    "# Matching Pennies\n",
    "mp = nash.Game(np.array([[1, -1], [-1, 1]]), np.array([[-1, 1], [1, -1]]))\n",
    "\n",
    "# Battle of the Sexes\n",
    "bos = nash.Game(np.array([[1, 0], [0, 2]]), np.array([[2, 0], [0, 1]]))\n",
    "\n",
    "games = {\n",
    "    \"Prisoner's dilemma\": pd,\n",
    "    \"Prisoner's dilemma (negative pay-offs)\": pd_negative,\n",
    "    \"Game of Chicken\": chicken,\n",
    "    \"Stug Hut\": hunt,\n",
    "    \"Matching Pennies\": mp,\n",
    "    \"Battle of the Sexes\": bos}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure-functions definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_of_anarchy(game):\n",
    "    # nash equilibria\n",
    "    eqs = list(game.support_enumeration())\n",
    "\n",
    "    # list of expected utilities for both players for pure strategies\n",
    "    pure_outcomes = [game[s_row, s_col]\n",
    "                     for s_row in pure_strategies_2b2 for s_col in pure_strategies_2b2]\n",
    "\n",
    "    # list of expected utilities for both players in nash equilibria\n",
    "    equil_outcomes = [game[s_row, s_col]\n",
    "                      for (s_row, s_col) in eqs]\n",
    "\n",
    "    # we only need to check pure strategies because the maximum is\n",
    "    # necessarily among these\n",
    "    # maximum expected payoff strategy\n",
    "    max_strat = np.argmax(list(map(sum, pure_outcomes)))\n",
    "\n",
    "    # maximum expected payoff nash equil\n",
    "    min_in_equil = np.argmin(list(map(sum, equil_outcomes)))\n",
    "\n",
    "    return sum(pure_outcomes[max_strat]) / sum(equil_outcomes[min_in_equil])\n",
    "\n",
    "\n",
    "def welf_regret(game):\n",
    "    # nash equilibria\n",
    "    eqs = list(game.support_enumeration())\n",
    "\n",
    "    # list of expected utilities for both players for pure strategies\n",
    "    pure_outcomes = [game[s_row, s_col]\n",
    "                     for s_row in pure_strategies_2b2 for s_col in pure_strategies_2b2]\n",
    "\n",
    "    # list of expected utilities for both players in nash equilibria\n",
    "    equil_outcomes = [game[s_row, s_col]\n",
    "                      for (s_row, s_col) in eqs]\n",
    "\n",
    "    # we only need to check pure strategies because the maximum is\n",
    "    # necessarily among these\n",
    "    max_strat = np.argmax(list(map(sum, pure_outcomes)))\n",
    "\n",
    "    min_in_equil = np.argmin(list(map(sum, equil_outcomes)))\n",
    "\n",
    "    return sum(pure_outcomes[max_strat]) - sum(equil_outcomes[min_in_equil])\n",
    "\n",
    "\n",
    "def princ_welf_regret(principal_game, agents_game):\n",
    "    # nash equilibria\n",
    "    eqs = list(agents_game.support_enumeration())\n",
    "\n",
    "    # list of expected utilities for both PRINCIPALS for pure strategies\n",
    "    pure_outcomes = [principal_game[s_row, s_col]\n",
    "                     for s_row in pure_strategies_2b2 for s_col in pure_strategies_2b2]\n",
    "\n",
    "    # list of expected utilities for both PRINCIPALS in nash equilibria of AGENT GAME\n",
    "    equil_outcomes = [principal_game[s_row, s_col]\n",
    "                      for (s_row, s_col) in eqs]\n",
    "\n",
    "    # we only need to check pure strategies because the maximum is\n",
    "    # necessarily among these\n",
    "    max_strat = np.argmax(list(map(sum, pure_outcomes)))\n",
    "\n",
    "    min_in_equil = np.argmin(list(map(sum, equil_outcomes)))\n",
    "\n",
    "    return sum(pure_outcomes[max_strat]) - sum(equil_outcomes[min_in_equil])\n",
    "\n",
    "\n",
    "def epic_horiz(game):\n",
    "    # for measuring horizontal alignment\n",
    "    row_payoffs = game.payoff_matrices[0]\n",
    "    col_payoffs = game.payoff_matrices[1]\n",
    "\n",
    "    # note: uniform distribution assumed\n",
    "    matrix_coeffs = np.corrcoef([row_payoffs.flatten(), col_payoffs.flatten()])\n",
    "    return matrix_coeffs[0][1]\n",
    "\n",
    "\n",
    "def epic_vertic(principal_game, agents_game):\n",
    "    # for measuring vertical alignment\n",
    "\n",
    "    row_payoffs = principal_game.payoff_matrices[0]\n",
    "    col_payoffs = principal_game.payoff_matrices[1]\n",
    "\n",
    "    agent_row_payoffs = agents_game.payoff_matrices[0]\n",
    "    agent_col_payoffs = agents_game.payoff_matrices[1]\n",
    "\n",
    "    # note: uniform distribution assumed\n",
    "    row_matrix_coeffs = np.corrcoef(\n",
    "        [row_payoffs.flatten(), agent_row_payoffs.flatten()])\n",
    "    col_matrix_coeffs = np.corrcoef(\n",
    "        [col_payoffs.flatten(), agent_col_payoffs.flatten()])\n",
    "    return (row_matrix_coeffs[0][1], col_matrix_coeffs[0][1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb(principal_game):\n",
    "    row_payoffs = principal_game.payoff_matrices[0]\n",
    "    col_payoffs = principal_game.payoff_matrices[1]\n",
    "\n",
    "    pert_row_payoffs = np.array([[0, 0], [0, 0]], dtype=float)\n",
    "    pert_col_payoffs = np.array([[0, 0], [0, 0]], dtype=float)\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            payoff = row_payoffs[i][j]\n",
    "            x = random.uniform(-1, 2)\n",
    "            pert_row_payoffs[i][j] = x * payoff\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            payoff = col_payoffs[i][j]\n",
    "            x = random.uniform(-1, 2)\n",
    "            pert_col_payoffs[i][j] = x * payoff\n",
    "\n",
    "    agent_game = nash.Game(pert_row_payoffs, pert_col_payoffs)\n",
    "\n",
    "    return agent_game"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resutls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRINCIPALS\n",
      "Bi matrix game with payoff matrices:\n",
      "\n",
      "Row player:\n",
      "[[1 0]\n",
      " [0 2]]\n",
      "\n",
      "Column player:\n",
      "[[2 0]\n",
      " [0 1]]\n",
      "AGENTS\n",
      "Bi matrix game with payoff matrices:\n",
      "\n",
      "Row player:\n",
      "[[-0.3528019   0.        ]\n",
      " [ 0.          1.30950457]]\n",
      "\n",
      "Column player:\n",
      "[[ 0.1965057   0.        ]\n",
      " [ 0.         -0.05428649]]\n",
      "Horizontal alignment between agents: -0.7194075044884151\n",
      "Vertical alignment between row principal & agent: 0.7359047346882436\n",
      "Vertical alignment between column principal & agent: 0.7324181224419363\n",
      "Welfare regret: 1.2552180807360689\n",
      "Cross-game regret: 3.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_measures_and_print(principal_game, agents_game):\n",
    "    print(\"PRINCIPALS\")\n",
    "    print(principal_game)\n",
    "    print(\"AGENTS\")\n",
    "    print(agents_game)\n",
    "    print(\"Horizontal alignment between agents: \" + str(epic_horiz(agents_game)))\n",
    "    row_align, col_align = epic_vertic(principal_game, agents_game)\n",
    "    print(\"Vertical alignment between row principal & agent: \" + str(row_align))\n",
    "    print(\"Vertical alignment between column principal & agent: \" + str(col_align))\n",
    "    print(\"Welfare regret: \" + str(welf_regret(agents_game)))\n",
    "    print(\"Cross-game regret: \" + str(princ_welf_regret(principal_game, agents_game)))\n",
    "    print(\"\")\n",
    "\n",
    "calculate_measures_and_print(bos, perturb(bos))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
